experiment_name: baseline
output_dir: outputs/baseline
seed: 42
device: cuda

model:
  model_type: llm
  model_name: meta-llama/Llama-2-7b-hf
  temperature: 0.0
  max_tokens: 2048
  use_code_generation: false
  use_llm_reasoning: true

training:
  batch_size: 4
  learning_rate: 1e-5
  num_epochs: 3
  warmup_steps: 100
  weight_decay: 0.01
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0

data:
  data_dir: data/raw
  train_split: 0.9
  max_seq_length: 2048
  preprocessing:
    normalize_latex: true
    extract_math: true

