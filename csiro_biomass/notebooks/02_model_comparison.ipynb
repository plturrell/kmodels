{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - Model Comparison\n",
    "\n",
    "This notebook compares different model configurations and architectures to help you choose the best approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan outputs directory for completed runs\n",
    "outputs_dir = Path.cwd().parent / \"outputs\" / \"baseline\"\n",
    "\n",
    "results = []\n",
    "for run_dir in outputs_dir.glob(\"run-*\"):\n",
    "    metrics_file = run_dir / \"validation_constraint_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Extract run info from directory name\n",
    "        run_name = run_dir.name\n",
    "        \n",
    "        results.append({\n",
    "            'run': run_name,\n",
    "            'rmse_raw': metrics.get('rmse_raw', None),\n",
    "            'rmse_repaired': metrics.get('rmse_repaired', None),\n",
    "            'mae_raw': metrics.get('mae_raw', None),\n",
    "            'mae_repaired': metrics.get('mae_repaired', None),\n",
    "            'improvement': metrics.get('rmse_raw', 0) - metrics.get('rmse_repaired', 0),\n",
    "            **{f'rmse_{k}': v for k, v in metrics.get('per_target_rmse', {}).items()}\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(f\"Found {len(df_results)} completed runs\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_results) > 0:\n",
    "    # Sort by RMSE (repaired)\n",
    "    df_sorted = df_results.sort_values('rmse_repaired')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # RMSE comparison\n",
    "    x = range(len(df_sorted))\n",
    "    axes[0].bar(x, df_sorted['rmse_raw'], alpha=0.5, label='Raw', color='lightcoral')\n",
    "    axes[0].bar(x, df_sorted['rmse_repaired'], alpha=0.8, label='Repaired', color='steelblue')\n",
    "    axes[0].set_xlabel('Run')\n",
    "    axes[0].set_ylabel('RMSE (g)')\n",
    "    axes[0].set_title('RMSE Comparison Across Runs')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(df_sorted['run'], rotation=45, ha='right')\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[1].bar(x, df_sorted['mae_raw'], alpha=0.5, label='Raw', color='lightcoral')\n",
    "    axes[1].bar(x, df_sorted['mae_repaired'], alpha=0.8, label='Repaired', color='forestgreen')\n",
    "    axes[1].set_xlabel('Run')\n",
    "    axes[1].set_ylabel('MAE (g)')\n",
    "    axes[1].set_title('MAE Comparison Across Runs')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(df_sorted['run'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBest Run (by RMSE repaired):\")\n",
    "    best_run = df_sorted.iloc[0]\n",
    "    print(f\"  Run: {best_run['run']}\")\n",
    "    print(f\"  RMSE (repaired): {best_run['rmse_repaired']:.2f} g\")\n",
    "    print(f\"  MAE (repaired): {best_run['mae_repaired']:.2f} g\")\n",
    "    print(f\"  Improvement from constraint repair: {best_run['improvement']:.2f} g\")\n",
    "else:\n",
    "    print(\"No completed runs found. Train a model first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Target Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_results) > 0:\n",
    "    # Extract per-target RMSE columns\n",
    "    target_cols = [col for col in df_results.columns if col.startswith('rmse_Dry') or col.startswith('rmse_GDM')]\n",
    "    \n",
    "    if target_cols:\n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Prepare data for heatmap\n",
    "        heatmap_data = df_results[['run'] + target_cols].set_index('run')\n",
    "        heatmap_data.columns = [col.replace('rmse_', '') for col in heatmap_data.columns]\n",
    "        \n",
    "        sns.heatmap(heatmap_data.T, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "                   cbar_kws={'label': 'RMSE (g)'}, linewidths=0.5)\n",
    "        plt.title('Per-Target RMSE Across Runs')\n",
    "        plt.xlabel('Run')\n",
    "        plt.ylabel('Target Variable')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nPer-Target RMSE Summary:\")\n",
    "        print(heatmap_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint Repair Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_results) > 0 and 'improvement' in df_results.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    improvement_pct = (df_results['improvement'] / df_results['rmse_raw'] * 100)\n",
    "    \n",
    "    plt.bar(range(len(df_results)), improvement_pct, color='green', alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Improvement (%)')\n",
    "    plt.title('RMSE Improvement from Constraint Repair')\n",
    "    plt.xticks(range(len(df_results)), df_results['run'], rotation=45, ha='right')\n",
    "    plt.axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAverage improvement: {improvement_pct.mean():.2f}%\")\n",
    "    print(f\"Max improvement: {improvement_pct.max():.2f}%\")\n",
    "    print(f\"Min improvement: {improvement_pct.min():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "Based on the results above, here are configuration recommendations for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1. QUICK EXPERIMENTATION (5-10 min)\")\n",
    "print(\"   --max_epochs 10 --batch_size 16 --backbone resnet18\")\n",
    "print(\"\\n2. BALANCED PERFORMANCE (20-30 min)\")\n",
    "print(\"   --max_epochs 50 --batch_size 8 --backbone efficientnet_b3\")\n",
    "print(\"\\n3. BEST PERFORMANCE (1-2 hours)\")\n",
    "print(\"   --max_epochs 100 --batch_size 4 --backbone efficientnet_b4\")\n",
    "print(\"\\n4. CROSS-VALIDATION (2-4 hours)\")\n",
    "print(\"   --max_epochs 50 --num_folds 5\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

